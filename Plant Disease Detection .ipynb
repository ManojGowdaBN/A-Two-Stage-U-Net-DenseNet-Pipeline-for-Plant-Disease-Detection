{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5e2a2c-7703-4f4b-ad05-c8c99f806b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import cv2\n",
    "from skimage import filters, feature\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import os\n",
    "import glob\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44534072-ed89-46fc-a67e-0fcf9a8e197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bacterial_Spot: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:01<00:00, 128.89it/s]\n",
      "Black_Measles: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 197.92it/s]\n",
      "Black_Rot: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 100.24it/s]\n",
      "Gray_Leaf_Spot: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.62it/s]\n",
      "Healthy: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 390/390 [00:12<00:00, 31.84it/s]\n",
      "Powdery: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:09<00:00, 30.84it/s]\n",
      "Rust: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:09<00:00, 31.50it/s]\n",
      "Scab: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:02<00:00, 114.22it/s]\n",
      "Bacterial_Spot: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 171.88it/s]\n",
      "Black_Measles: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 191.62it/s]\n",
      "Black_Rot: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 201.89it/s]\n",
      "Gray_Leaf_Spot: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 158.49it/s]\n",
      "Healthy: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  9.52it/s]\n",
      "Powdery: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 11.60it/s]\n",
      "Rust: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 10.71it/s]\n",
      "Scab: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 242.55it/s]\n",
      "Bacterial_Spot: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:01<00:00, 27.82it/s]\n",
      "Black_Measles: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 77.89it/s]\n",
      "Black_Rot: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 36.43it/s]\n",
      "Gray_Leaf_Spot: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 37.68it/s]\n",
      "Healthy: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:06<00:00,  7.84it/s]\n",
      "Powdery: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:06<00:00,  7.40it/s]\n",
      "Rust: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:06<00:00,  7.64it/s]\n",
      "Scab: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 35.01it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input/Output directories\n",
    "dataset_path = r\"E:\\plant dieaseas\"\n",
    "train_dir = os.path.join(dataset_path, \"train\")\n",
    "val_dir = os.path.join(dataset_path, \"validation\")\n",
    "test_dir = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "mask_base_dir = os.path.join(dataset_path, \"masks\")\n",
    "os.makedirs(mask_base_dir, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "def generate_leaf_mask(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"[ERROR] Could not read image: {image_path}\")\n",
    "        return None  # Skip this image\n",
    "\n",
    "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower = np.array([20, 40, 40])\n",
    "    upper = np.array([85, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest = max(contours, key=cv2.contourArea)\n",
    "        clean_mask = np.zeros_like(mask)\n",
    "        cv2.drawContours(clean_mask, [largest], -1, 255, thickness=cv2.FILLED)\n",
    "        return clean_mask\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Generate masks for entire dataset\n",
    "def generate_masks_for_dir(image_dir, save_mask_dir):\n",
    "    os.makedirs(save_mask_dir, exist_ok=True)\n",
    "    for class_folder in os.listdir(image_dir):\n",
    "        class_input_dir = os.path.join(image_dir, class_folder)\n",
    "        class_mask_dir = os.path.join(save_mask_dir, class_folder)\n",
    "        os.makedirs(class_mask_dir, exist_ok=True)\n",
    "\n",
    "        for filename in tqdm(os.listdir(class_input_dir), desc=class_folder):\n",
    "            # Skip non-image files\n",
    "            if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(class_input_dir, filename)\n",
    "            mask = generate_leaf_mask(img_path)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask_path = os.path.join(class_mask_dir, filename)\n",
    "                cv2.imwrite(mask_path, mask)\n",
    "\n",
    "\n",
    "# Generate masks\n",
    "generate_masks_for_dir(train_dir, os.path.join(mask_base_dir, \"train\"))\n",
    "generate_masks_for_dir(val_dir, os.path.join(mask_base_dir, \"validation\"))\n",
    "generate_masks_for_dir(test_dir, os.path.join(mask_base_dir, \"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80d31cd-7f69-4255-8e89-7abd4ffef426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1940, 256, 256, 3), (1940, 256, 256, 1)\n",
      "Validation: (160, 256, 256, 3), (160, 256, 256, 1)\n",
      "Test: (400, 256, 256, 3), (400, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "# Base directories\n",
    "base_dir = r\"E:\\plant dieaseas\"\n",
    "image_train_dir = os.path.join(base_dir, \"train\")\n",
    "image_val_dir = os.path.join(base_dir, \"validation\")\n",
    "image_test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "mask_base_dir = os.path.join(base_dir, \"masks\")\n",
    "mask_train_dir = os.path.join(mask_base_dir, \"train\")\n",
    "mask_val_dir = os.path.join(mask_base_dir, \"validation\")\n",
    "mask_test_dir = os.path.join(mask_base_dir, \"test\")\n",
    "\n",
    "# Your function\n",
    "def load_images_and_masks(image_dir, mask_dir, img_size):\n",
    "    images = []\n",
    "    masks = []\n",
    "    for class_folder in os.listdir(image_dir):\n",
    "        img_class_path = os.path.join(image_dir, class_folder)\n",
    "        mask_class_path = os.path.join(mask_dir, class_folder)\n",
    "\n",
    "        if not os.path.isdir(img_class_path) or not os.path.isdir(mask_class_path):\n",
    "            continue  # skip non-folder entries\n",
    "\n",
    "        for filename in os.listdir(img_class_path):\n",
    "            if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue  # skip non-image files\n",
    "\n",
    "            img_path = os.path.join(img_class_path, filename)\n",
    "            mask_path = os.path.join(mask_class_path, filename)\n",
    "\n",
    "            if not os.path.exists(mask_path):\n",
    "                print(f\"[WARNING] No mask for: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            if image is None:\n",
    "                print(f\"[ERROR] Could not read image: {img_path}\")\n",
    "                continue\n",
    "            if mask is None:\n",
    "                print(f\"[ERROR] Could not read mask: {mask_path}\")\n",
    "                continue\n",
    "\n",
    "            image = cv2.resize(image, (img_size, img_size)) / 255.0\n",
    "            mask = cv2.resize(mask, (img_size, img_size)) / 255.0\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "            images.append(image)\n",
    "            masks.append(mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_images, train_masks = load_images_and_masks(image_train_dir, mask_train_dir, IMG_SIZE)\n",
    "val_images, val_masks = load_images_and_masks(image_val_dir, mask_val_dir, IMG_SIZE)\n",
    "test_images, test_masks = load_images_and_masks(image_test_dir, mask_test_dir, IMG_SIZE)\n",
    "\n",
    "print(f\"Train: {train_images.shape}, {train_masks.shape}\")\n",
    "print(f\"Validation: {val_images.shape}, {val_masks.shape}\")\n",
    "print(f\"Test: {test_images.shape}, {test_masks.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebde7d6-965d-43f1-b054-e146a3c04aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_ultra_light_unet(input_shape=(128, 128, 3)):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(16, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(16, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D()(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(32, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(32, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D()(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = layers.Conv2D(64, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(64, 3, activation='relu', padding='same')(c3)\n",
    "\n",
    "    # Decoder\n",
    "    u4 = layers.Conv2DTranspose(32, 2, strides=2, padding='same')(c3)\n",
    "    u4 = layers.concatenate([u4, c2])\n",
    "    c4 = layers.Conv2D(32, 3, activation='relu', padding='same')(u4)\n",
    "    c4 = layers.Conv2D(32, 3, activation='relu', padding='same')(c4)\n",
    "\n",
    "    u5 = layers.Conv2DTranspose(16, 2, strides=2, padding='same')(c4)\n",
    "    u5 = layers.concatenate([u5, c1])\n",
    "    c5 = layers.Conv2D(16, 3, activation='relu', padding='same')(u5)\n",
    "    c5 = layers.Conv2D(16, 3, activation='relu', padding='same')(c5)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(c5)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c52e67-2b47-4ef8-8ae3-139c267f59d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 183ms/step - accuracy: 0.7752 - loss: 0.3476 - val_accuracy: 0.8904 - val_loss: 0.1539\n",
      "Epoch 2/5\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 174ms/step - accuracy: 0.8946 - loss: 0.1222 - val_accuracy: 0.9001 - val_loss: 0.1081\n",
      "Epoch 3/5\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 176ms/step - accuracy: 0.8960 - loss: 0.1101 - val_accuracy: 0.8981 - val_loss: 0.1029\n",
      "Epoch 4/5\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 180ms/step - accuracy: 0.9009 - loss: 0.0869 - val_accuracy: 0.9027 - val_loss: 0.0977\n",
      "Epoch 5/5\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 146ms/step - accuracy: 0.8999 - loss: 0.0919 - val_accuracy: 0.9013 - val_loss: 0.0886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x209a7035990>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_ultra_light_unet(input_shape=(128, 128, 3))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# If needed: resize your data\n",
    "train_images_resized = tf.image.resize(train_images, (128, 128))\n",
    "train_masks_resized = tf.image.resize(train_masks, (128, 128))\n",
    "val_images_resized = tf.image.resize(val_images, (128, 128))\n",
    "val_masks_resized = tf.image.resize(val_masks, (128, 128))\n",
    "\n",
    "# Train\n",
    "model.fit(train_images_resized, train_masks_resized,\n",
    "          validation_data=(val_images_resized, val_masks_resized),\n",
    "          epochs=5, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7229ae0c-adda-4ea0-80cb-a1d36ba53dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"unet_segmentation_model1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d358ba-9b3b-47ab-900f-3f380bb4127d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1940 files belonging to 8 classes.\n",
      "Found 160 files belonging to 8 classes.\n",
      "Found 400 files belonging to 8 classes.\n",
      "Epoch 1/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1941 - loss: 2.2767"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 3s/step - accuracy: 0.1948 - loss: 2.2736 - val_accuracy: 0.5938 - val_loss: 1.6870 - learning_rate: 1.0000e-04\n",
      "Epoch 2/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4074 - loss: 1.6075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 3s/step - accuracy: 0.4079 - loss: 1.6067 - val_accuracy: 0.7000 - val_loss: 1.1895 - learning_rate: 1.0000e-04\n",
      "Epoch 3/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5498 - loss: 1.2907"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 3s/step - accuracy: 0.5501 - loss: 1.2900 - val_accuracy: 0.7688 - val_loss: 0.7810 - learning_rate: 1.0000e-04\n",
      "Epoch 4/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6264 - loss: 1.0676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 3s/step - accuracy: 0.6267 - loss: 1.0669 - val_accuracy: 0.8375 - val_loss: 0.5401 - learning_rate: 1.0000e-04\n",
      "Epoch 5/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7369 - loss: 0.7884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 3s/step - accuracy: 0.7370 - loss: 0.7880 - val_accuracy: 0.8875 - val_loss: 0.3942 - learning_rate: 1.0000e-04\n",
      "Epoch 6/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7844 - loss: 0.6459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 3s/step - accuracy: 0.7846 - loss: 0.6453 - val_accuracy: 0.9062 - val_loss: 0.3117 - learning_rate: 1.0000e-04\n",
      "Epoch 7/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8397 - loss: 0.4947"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 3s/step - accuracy: 0.8396 - loss: 0.4948 - val_accuracy: 0.9312 - val_loss: 0.2243 - learning_rate: 1.0000e-04\n",
      "Epoch 8/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8630 - loss: 0.4117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 3s/step - accuracy: 0.8630 - loss: 0.4116 - val_accuracy: 0.9563 - val_loss: 0.2011 - learning_rate: 1.0000e-04\n",
      "Epoch 9/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8789 - loss: 0.3681"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 3s/step - accuracy: 0.8790 - loss: 0.3679 - val_accuracy: 0.9563 - val_loss: 0.1617 - learning_rate: 1.0000e-04\n",
      "Epoch 10/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9077 - loss: 0.2976"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 3s/step - accuracy: 0.9076 - loss: 0.2975 - val_accuracy: 0.9500 - val_loss: 0.1584 - learning_rate: 1.0000e-04\n",
      "Epoch 11/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9053 - loss: 0.2679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 3s/step - accuracy: 0.9053 - loss: 0.2681 - val_accuracy: 0.9500 - val_loss: 0.1462 - learning_rate: 1.0000e-04\n",
      "Epoch 12/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 3s/step - accuracy: 0.9336 - loss: 0.2118 - val_accuracy: 0.9375 - val_loss: 0.1607 - learning_rate: 1.0000e-04\n",
      "Epoch 13/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9217 - loss: 0.2379"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 3s/step - accuracy: 0.9218 - loss: 0.2376 - val_accuracy: 0.9688 - val_loss: 0.1200 - learning_rate: 1.0000e-04\n",
      "Epoch 14/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - accuracy: 0.9420 - loss: 0.1957 - val_accuracy: 0.9688 - val_loss: 0.1229 - learning_rate: 1.0000e-04\n",
      "Epoch 15/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9434 - loss: 0.1672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 3s/step - accuracy: 0.9433 - loss: 0.1675 - val_accuracy: 0.9688 - val_loss: 0.1029 - learning_rate: 1.0000e-04\n",
      "Epoch 16/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9569 - loss: 0.1511"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 3s/step - accuracy: 0.9569 - loss: 0.1511 - val_accuracy: 0.9688 - val_loss: 0.0894 - learning_rate: 1.0000e-04\n",
      "Epoch 17/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 3s/step - accuracy: 0.9462 - loss: 0.1864 - val_accuracy: 0.9812 - val_loss: 0.0902 - learning_rate: 1.0000e-04\n",
      "Epoch 18/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 3s/step - accuracy: 0.9540 - loss: 0.1456 - val_accuracy: 0.9625 - val_loss: 0.1081 - learning_rate: 1.0000e-04\n",
      "Epoch 19/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 3s/step - accuracy: 0.9603 - loss: 0.1209 - val_accuracy: 0.9625 - val_loss: 0.0895 - learning_rate: 1.0000e-04\n",
      "Epoch 20/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9668 - loss: 0.1105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 3s/step - accuracy: 0.9667 - loss: 0.1106 - val_accuracy: 0.9750 - val_loss: 0.0824 - learning_rate: 5.0000e-05\n",
      "Epoch 21/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - accuracy: 0.9670 - loss: 0.0903 - val_accuracy: 0.9688 - val_loss: 0.0874 - learning_rate: 5.0000e-05\n",
      "Epoch 22/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 3s/step - accuracy: 0.9656 - loss: 0.1013 - val_accuracy: 0.9688 - val_loss: 0.0942 - learning_rate: 5.0000e-05\n",
      "Epoch 23/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - accuracy: 0.9733 - loss: 0.0890 - val_accuracy: 0.9625 - val_loss: 0.1000 - learning_rate: 5.0000e-05\n",
      "Epoch 24/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 3s/step - accuracy: 0.9696 - loss: 0.0854 - val_accuracy: 0.9688 - val_loss: 0.1037 - learning_rate: 2.5000e-05\n",
      "Epoch 25/25\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 3s/step - accuracy: 0.9762 - loss: 0.0847 - val_accuracy: 0.9688 - val_loss: 0.0970 - learning_rate: 2.5000e-05\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - accuracy: 0.9673 - loss: 0.1154\n",
      "Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Step 1: Load raw datasets\n",
    "raw_train_ds = image_dataset_from_directory(\"E:/plant dieaseas/train\", image_size=(224, 224), batch_size=32, label_mode='categorical')\n",
    "raw_val_ds = image_dataset_from_directory(\"E:/plant dieaseas/validation\", image_size=(224, 224), batch_size=32, label_mode='categorical')\n",
    "raw_test_ds = image_dataset_from_directory(\"E:/plant dieaseas/test\", image_size=(224, 224), batch_size=32, label_mode='categorical')\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Step 2: Data augmentation + normalization\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_ds = raw_train_ds.map(lambda x, y: (normalization_layer(data_augmentation(x)), y))\n",
    "val_ds = raw_val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = raw_test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# ✅ Step 3: Add prefetch to datasets\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Step 4: Define and partially unfreeze DenseNet121\n",
    "base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Step 5: Build model\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=True)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 7: Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(\"best_densenet121_model_finetuned.h5\", save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Step 8: Train model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=25,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "# Step 9: Evaluate\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4b9f9-49c8-4634-bc82-71b8e36afc85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
